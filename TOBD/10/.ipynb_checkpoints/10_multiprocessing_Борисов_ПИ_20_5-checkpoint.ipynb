{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Параллельные вычисления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n",
    "\n",
    "Материалы:\n",
    "* Макрушин С.В. Лекция \"Параллельные вычисления\"\n",
    "* https://docs.python.org/3/library/multiprocessing.html\n",
    "    * https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process\n",
    "    * https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool\n",
    "    * https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
    "* https://numpy.org/doc/stable/reference/generated/numpy.array_split.html\n",
    "* https://nalepae.github.io/pandarallel/\n",
    "    * https://github.com/nalepae/pandarallel/blob/master/docs/examples_windows.ipynb\n",
    "    * https://github.com/nalepae/pandarallel/blob/master/docs/examples_mac_linux.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandarallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Посчитайте, сколько раз встречается буква \"a\" в файлах [\"xaa\", \"xab\", \"xac\", \"xad\"]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "files = [f\"{name}.txt\" for name in [\"xaa\", \"xab\", \"xac\", \"xad\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Выведите на экран слова из файла words_alpha, в которых есть две или более буквы \"e\" подряд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "words = (\n",
    "    pd.read_csv(\"10_multiprocessing_data/words_alpha.txt\", header=None)[0]\n",
    "    .dropna()\n",
    "    .sample(frac=1, replace=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__При решении данных задач не подразумевается использования циклов или генераторов Python в ходе работы с пакетами `numpy` и `pandas`, если в задании не сказано обратного. Решения задач, в которых для обработки массивов `numpy` или структур `pandas` используются явные циклы (без согласования с преподавателем), могут быть признаны некорректными и не засчитаны.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. В каждой строке файла `tag_nsteps.csv` хранится информация о тэге рецепта и количестве шагов в этом рецепте в следующем виде:\n",
    "\n",
    "```\n",
    "tags,n_steps\n",
    "hungarian,2\n",
    "european,6\n",
    "occasion,4\n",
    "pumpkin,4\n",
    "................\n",
    "```\n",
    "\n",
    "Всего в исходном файле хранится чуть меньше, чем 71 млн, строк. Разбейте файл `tag_nsteps.csv` на несколько (например, 8) примерно одинаковых по объему файлов c названиями `tag_nsteps_*.csv`, где вместо символа `*` указан номер очередного файла. Каждый файл имеет структуру, аналогичную оригинальному файлу (включая заголовок).\n",
    "\n",
    "__Важно__: здесь и далее вы не можете загружать в память весь исходный файл сразу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(f'tag_nsteps.csv', chunksize=71_000_000//n_files)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.to_csv(f'tag_nsteps_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Напишите функцию, которая принимает на вход название файла, созданного в результате решения задачи 1, считает для каждого тэга сумму по столбцу `n_steps` и количество строк c этим тэгом, и возвращает результат в виде словаря. Ожидаемый вид итогового словаря:\n",
    "\n",
    "```\n",
    "{\n",
    "    '1-day-or-more': {'sum': 56616, 'count': 12752},\n",
    "    '15-minutes-or-less': {'sum': 195413, 'count': 38898},\n",
    "    '3-steps-or-less': {'sum': 187938, 'count': 39711},\n",
    "    ....\n",
    "}\n",
    "```\n",
    "\n",
    "Примените данную функцию к каждому файлу, полученному в задании 1, и соберите результат в виде списка словарей. Не используйте параллельных вычислений. \n",
    "\n",
    "Выведите на экран значение по ключу \"30-minutes-or-less\" для каждого из словарей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_sum_count_from_file(file: str) -> dict:\n",
    "    df = pd.read_csv(file)\n",
    "    df2 = df.groupby(\"tags\")[\"n_steps\"]\n",
    "    df3 = df2.agg(sum = 'sum', count = 'count')\n",
    "    return df3.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = [get_tag_sum_count_from_file(f\"tag_nsteps_{i}.csv\") for i in range(n_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sum': 234016, 'count': 30597}\n",
      "{'sum': 230889, 'count': 30304}\n",
      "{'sum': 234232, 'count': 30757}\n",
      "{'sum': 237432, 'count': 31043}\n",
      "{'sum': 230152, 'count': 30297}\n",
      "{'sum': 233737, 'count': 30576}\n",
      "{'sum': 229742, 'count': 30242}\n",
      "{'sum': 234179, 'count': 30765}\n",
      "{'sum': 232940, 'count': 30657}\n",
      "{'sum': 231994, 'count': 30647}\n",
      "{'sum': 232735, 'count': 30735}\n",
      "{'sum': 221157, 'count': 29162}\n"
     ]
    }
   ],
   "source": [
    "for dict_ in res2:\n",
    "    print(dict_[\"30-minutes-or-less\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Напишите функцию, которая объединяет результаты обработки отдельных файлов. Данная функция принимает на вход список словарей, каждый из которых является результатом вызова функции `get_tag_sum_count_from_file` для конкретного файла, и агрегирует эти словари. Не используйте параллельных вычислений.\n",
    "\n",
    "Процедура агрегации словарей имеет следующий вид:\n",
    "$$d_{agg}[k] = \\{sum: \\sum_{i=1}^{n}d_{i}[k][sum], count: \\sum_{i=1}^{n}d_{i}[k][count]\\}$$\n",
    "где $d_1, d_2, ..., d_n$- результат вызова функции `get_tag_sum_count_from_file` для конкретных файлов.\n",
    "\n",
    "Примените данную функцию к результату выполнения задания 2. Выведите на экран результат для тэга \"30-minutes-or-less\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_results(tag_sum_count_list: list) -> dict:\n",
    "    main_df = pd.DataFrame(columns=[\"index\", \"sum\", \"count\"])\n",
    "    for dict_ in tag_sum_count_list:\n",
    "        temp = pd.DataFrame.from_dict(dict_, orient=\"index\").reset_index()\n",
    "        main_df = pd.concat([temp, main_df])\n",
    "        res = main_df.groupby(\"index\").sum().to_dict(\"index\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = agg_results(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sum': 2783205, 'count': 365782}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3[\"30-minutes-or-less\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Напишите функцию, которая считает среднее значение количества шагов для каждого тэга в словаре, имеющего вид, аналогичный словарям в задаче 2, и возвращает результат в виде словаря . Используйте решения задач 1-3, чтобы получить среднее значение количества шагов каждого тэга для всего датасета, имея результаты обработки частей датасета и результат их агрегации. Выведите на экран результат для тэга \"30-minutes-or-less\".\n",
    "\n",
    "Определите, за какое время задача решается для всего датасета. При замере времени учитывайте время расчета статистики для каждого файла, агрегации результатов и, собственно, вычисления средного. Временем, затрачиваемым на процедуру разбиения исходного файла можно пренебречь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_mean_n_steps(tag_sum_count: dict) -> dict:\n",
    "    for tag,sum_count in tag_sum_count.items():\n",
    "        tag_sum_count[tag] = sum_count[\"sum\"] / sum_count[\"count\"]\n",
    "    return tag_sum_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 s ± 37.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "res4 = get_tag_mean_n_steps(agg_results([get_tag_sum_count_from_file(f\"tag_nsteps_{i}.csv\") for i in range(n_files)]))\n",
    "res4[\"30-minutes-or-less\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.608917333275011"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res4 = get_tag_mean_n_steps(agg_results([get_tag_sum_count_from_file(f\"tag_nsteps_{i}.csv\") for i in range(n_files)]))\n",
    "res4[\"30-minutes-or-less\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Повторите решение задачи 4, распараллелив вызовы функции `get_tag_sum_count_from_file` для различных файлов с помощью `multiprocessing.Pool`. Для обработки каждого файла создайте свой собственный процесс. Выведите на экран результат для тэга \"30-minutes-or-less\". Определите, за какое время задача решается для всех файлов. При замере времени учитывайте время расчета статистики для каждого файла, агрегации результатов и, собственно, вычисления средного. Временем, затрачиваемым на процедуру разбиения исходного файла можно пренебречь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helpers.py\n"
     ]
    }
   ],
   "source": [
    "%%file helpers.py\n",
    "import pandas as pd\n",
    "def get_tag_sum_count_from_file(file: str) -> dict:\n",
    "    df = pd.read_csv(file)\n",
    "    df2 = df.groupby(\"tags\")[\"n_steps\"]\n",
    "    df3 = df2.agg(sum = 'sum', count = 'count')\n",
    "    return df3.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.28 s ± 32.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pool = mp.Pool(processes=n_files)\n",
    "results = list(pool.map(helpers.get_tag_sum_count_from_file, [f'tag_nsteps_{i}.csv' for i in range(n_files)]))\n",
    "res5 = get_tag_mean_n_steps(agg_results(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.608917333275011"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = mp.Pool(processes=n_files)\n",
    "results = list(pool.map(helpers.get_tag_sum_count_from_file, [f'tag_nsteps_{i}.csv' for i in range(n_files)]))\n",
    "res5 = get_tag_mean_n_steps(agg_results(results))\n",
    "res5[\"30-minutes-or-less\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Повторите решение задачи 4, распараллелив вычисления функции `get_tag_sum_count_from_file` для различных файлов с помощью `multiprocessing.Process`. Для обработки каждого файла создайте свой собственный процесс. Для обмена данными между процессами используйте `multiprocessing.Queue`.\n",
    "\n",
    "Выведите на экран результат для тэга \"30-minutes-or-less\". Определите, за какое время задача решается для всех файлов. При замере времени учитывайте время расчета статистики для каждого файла, агрегации результатов и, собственно, вычисления средного. Временем, затрачиваемым на процедуру разбиения исходного файла можно пренебречь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helpers2.py\n"
     ]
    }
   ],
   "source": [
    "%%file helpers2.py\n",
    "import pandas as pd\n",
    "\n",
    "def get_tag_sum_count_from_file(file: str, output) -> dict:\n",
    "    df = pd.read_csv(file)\n",
    "    df2 = df.groupby(\"tags\")[\"n_steps\"]\n",
    "    df3 = df2.agg(sum = 'sum', count = 'count')\n",
    "    output.put(df3.to_dict('index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3 s ± 28 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "output = mp.Queue()\n",
    "[mp.Process(target=helpers2.get_tag_sum_count_from_file, args=(f\"tag_nsteps_{i}.csv\", output)).start() \\\n",
    "        for i in range(n_files)]\n",
    "    \n",
    "results = [output.get() for _ in range(n_files)]\n",
    "res6 = get_tag_mean_n_steps(agg_results(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.608917333275011"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = mp.Queue()\n",
    "[mp.Process(target=helpers2.get_tag_sum_count_from_file, args=(f\"tag_nsteps_{i}.csv\", output)).start() \\\n",
    "        for i in range(n_files)]\n",
    "    \n",
    "results = [output.get() for _ in range(n_files)]\n",
    "res6 = get_tag_mean_n_steps(agg_results(results))\n",
    "res6[\"30-minutes-or-less\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Исследуйте, как влияет количество запущенных одновременно процессов на скорость решения задачи. Узнайте количество ядер вашего процессора $K$. Повторите решение задачи 1, разбив исходный файл на $\\frac{K}{2}$, $K$ и $2K$ фрагментов. Для каждого из разбиений повторите решение задачи 5. Визуализируйте зависимость времени выполнения кода от количества файлов в разбиении. Сделайте вывод в виде текстового комментария."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = mp.cpu_count()//2\n",
    "chunks = pd.read_csv(f'tag_nsteps.csv', chunksize=71_000_000//n_files)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.to_csv(f'tag_nsteps_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5 s ± 111 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pool = mp.Pool(processes=n_files)\n",
    "results = list(pool.map(helpers.get_tag_sum_count_from_file, [f'tag_nsteps_{i}.csv' for i in range(n_files)]))\n",
    "res5 = get_tag_mean_n_steps(agg_results(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = mp.cpu_count()\n",
    "chunks = pd.read_csv(f'tag_nsteps.csv', chunksize=71_000_000//n_files)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.to_csv(f'tag_nsteps_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.27 s ± 31.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pool = mp.Pool(processes=n_files)\n",
    "results = list(pool.map(helpers.get_tag_sum_count_from_file, [f'tag_nsteps_{i}.csv' for i in range(n_files)]))\n",
    "res5 = get_tag_mean_n_steps(agg_results(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = mp.cpu_count()*2\n",
    "chunks = pd.read_csv(f'tag_nsteps.csv', chunksize=71_000_000//n_files)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.to_csv(f'tag_nsteps_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.04 s ± 13.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pool = mp.Pool(processes=n_files)\n",
    "results = list(pool.map(helpers.get_tag_sum_count_from_file, [f'tag_nsteps_{i}.csv' for i in range(n_files)]))\n",
    "res5 = get_tag_mean_n_steps(agg_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K выгодный самый"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напишите функцию `parallel_map`, которая принимает на вход серию `s` `pd.Series` и функцию одного аргумента `f` и поэлементно применяет эту функцию к серии, распараллелив вычисления при помощи пакета `multiprocessing`. Логика работы функции `parallel_map` должна включать следующие действия:\n",
    "* разбиение исходной серии на $K$ частей, где $K$ - количество ядер вашего процессора;\n",
    "* параллельное применение функции `f` к каждой части при помощи метода _серии_ `map` при помощи нескольких подпроцессов;\n",
    "* объединение результатов работы подпроцессов в одну серию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helpers3.py\n"
     ]
    }
   ],
   "source": [
    "%%file helpers3.py\n",
    "import pandas as pd\n",
    "\n",
    "def mapp(s: pd.Series, f: callable) -> pd.Series:\n",
    "    return s.map(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(range(6*100))\n",
    "def parallel_map(s: pd.Series, f: callable) -> pd.Series: \n",
    "    lst = np.array_split(s, 6)\n",
    "    \n",
    "    pool = mp.Pool(processes=6)\n",
    "    return pd.concat(pool.starmap(helpers3.mapp, [(part, f) for part in lst] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Напишите функцию `f`, которая принимает на вход тэг и проверяет, удовлетворяет ли тэг следующему шаблону: `[любое число]-[любое слово]-or-less`. Возьмите любой фрагмент файла, полученный в задании 1, примените функцию `f` при помощи `parallel_map` к столбцу `tags` и посчитайте количество тэгов, подходящих под этот шаблон. Решите ту же задачу, воспользовавшись методом _серий_ `map`. Сравните время и результат выполнения двух решений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helpers4.py\n"
     ]
    }
   ],
   "source": [
    "%%file helpers4.py\n",
    "import re\n",
    "def f(tag: str) -> bool:\n",
    "    return bool(re.match(\"\\d+-\\w+-or-less\", str(tag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tag_nsteps_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01 s ± 15.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "parallel_map(df.tags, helpers4.f).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9 s ± 9.09 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "len(df.tags[df.tags.map(helpers4.f)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Используя пакет `pandarallel`, примените функцию `f` из задания 9 к столбцу `tags` таблицы, с которой вы работали этом задании. Посчитайте количество тэгов, подходящих под описанный шаблон. Измерьте время выполнения кода. Выведите на экран полученный результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38 s ± 23.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df.tags.parallel_map(helpers4.f).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# все таки разделение на потоки оказалось самым быстрым"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
